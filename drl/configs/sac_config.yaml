# SAC Training Configuration for Traffic Intersection DRL - ULTRA-OPTIMIZED MODE with FIXED TIMING

# Training Parameters - OPTIMIZED FOR 4 CRITICAL PARAMETERS
total_timesteps: 100000  # SAC is more sample efficient than PPO
eval_freq: 2000          # Evaluate every 2000 steps
checkpoint_freq: 5000    # Save checkpoints every 5000 steps

# SAC Algorithm Parameters - OPTIMIZED FOR 4 CRITICAL PARAMETERS
learning_rate: 3e-4      # Learning rate for both actor and critic
buffer_size: 1000000     # Experience replay buffer size (1M experiences)
batch_size: 256          # Batch size for training (larger than PPO for off-policy)
tau: 0.005               # Soft update coefficient for target networks
gamma: 0.99              # Discount factor
train_freq: 1            # Train every step (continuous learning)
gradient_steps: 1        # Number of gradient steps per training step
ent_coef: 'auto'         # Entropy coefficient (automatic tuning)
target_update_interval: 1 # Update target networks every step
learning_starts: 1000    # Start learning after 1000 random steps
use_sde: false           # Use State Dependent Exploration
use_action_noise: false  # Additional action noise (optional for SAC)

# Neural Network Architecture
policy_kwargs:
  log_std_init: -3       # Initial log std for policy
  net_arch: [256, 256]   # Hidden layers for actor and critic networks

# FIXED TIMING CONFIGURATION - Synchronized intervals for proper vehicle control
timing_config:
  # FIXED: New synchronized time hierarchy
  fixed_delta_seconds: 0.1          # Simulation step: 0.1s (10 FPS)
  logic_update_interval: 1.0        # Decision step: 1.0s (10 sim steps per decision)
  auction_interval: 4.0             # Auction cycle: 4.0s (4 decisions per cycle)
  bidding_duration: 2.0             # Bidding phase: 2.0s (50% of cycle)
  deadlock_check_interval: 8.0      # System check: 8.0s (2 auction cycles)
  
  # This ensures PERFECT synchronization:
  # - 10 sim steps per decision
  # - 4 decisions per auction cycle  
  # - Vehicles properly respect 'wait' commands
  # - No more "all vehicles moving together" issue

# Simulation Configuration - FASTER EPISODES FOR SAC with FIXED TIMING
sim_config:
  map: 'Town05'
  max_steps: 500         # Shorter episodes for SAC (off-policy can handle this better)
  spawn_rate: 0.8
  intersection_center: [-188.9, -89.7, 0.0]
  training_mode: true
  deadlock_reset_enabled: true
  deadlock_timeout_duration: 12.0  # Shorter timeout for SAC
  max_deadlock_resets: 3
  severe_deadlock_reset_enabled: true
  severe_deadlock_punishment: -300.0
  
  # FIXED: Apply new timing configuration
  fixed_delta_seconds: 0.1
  logic_update_interval_seconds: 1.0
  auction_interval: 4.0
  bidding_duration: 2.0

# Reward Configuration - FIXED VALUES (not trainable)
reward_config:
  vehicle_exit_reward: 10.0      # FIXED: Not trainable
  throughput_bonus: 0.01         # FIXED: Not trainable
  acceleration_penalty_threshold: 3.0
  acceleration_penalty_factor: 2.0
  efficiency_bonus: 5.0
  collision_penalty: 100.0       # FIXED: Not trainable
  deadlock_penalty: 800.0        # FIXED: Not trainable
  step_penalty: 0.05             # Lower step penalty for SAC

# ULTRA-OPTIMIZED: Only 4 trainable parameters instead of 8 parameters (50% reduction)
# These are the MOST IMPORTANT parameters for deadlock avoidance
nash_hyperparams:
  path_intersection_threshold:
    range: [2.5, 2.5]  # FIXED: Not trainable
    default: 2.5
    description: "Distance threshold for detecting path intersections (FIXED)"
  platoon_conflict_distance:
    range: [15.0, 15.0]  # FIXED: Not trainable
    default: 15.0
    description: "Distance at which platoons are considered in conflict (FIXED)"

# NEW: 紧急度与位置优势关系因子说明
# urgency_position_ratio: 控制紧急度vs位置优势的平衡关系
# - 高值 (>1.0): 优先考虑紧急度，从时间敏感车辆获得更高收入
# - 低值 (<1.0): 优先考虑位置优势，从在路口的车辆获得更高收入
# - 这个参数替换了原来的 bid_scale，提供更智能的收入最大化策略

# SAC Specific Parameters
sac_specific:
  use_target_entropy: true    # Use target entropy for automatic entropy tuning
  target_entropy: 'auto'     # Target entropy (auto = -action_dim)
  ent_coef_lr: 3e-4          # Learning rate for entropy coefficient
  critic_lr: 3e-4            # Critic learning rate (can be different from actor)
  actor_lr: 3e-4             # Actor learning rate

# Training Schedule - DIFFERENT FROM PPO
schedule:
  warmup_steps: 1000         # Random exploration steps
  exploration_steps: 10000   # Steps for initial exploration
  min_learning_rate: 1e-5    # Minimum learning rate

# Evaluation
evaluation:
  n_eval_episodes: 5
  eval_deterministic: true
  eval_render: false

# Action Space Optimization for SAC
action_space:
  # SAC works better with normalized action spaces
  normalize_actions: true
  action_bounds_penalty: 0.1  # Penalty for actions near bounds

# Logging
logging:
  tensorboard: false
  csv: true
  metrics_freq: 100
  save_freq: 2000

# FIXED TIMING NOTES:
# - New synchronized time hierarchy prevents vehicles from moving together
# - 1.0s decision intervals provide better control responsiveness
# - 4.0s auction cycles align perfectly with decision steps
# - 8.0s deadlock checks maintain system health monitoring
# - Vehicles now properly respect 'wait' commands due to synchronized timing
# - SAC vs PPO Differences:
#   1. SAC uses experience replay (buffer_size)
#   2. SAC is off-policy (can reuse old data)
#   3. SAC has automatic entropy tuning
#   4. SAC trains continuously (train_freq=1)
#   5. SAC uses soft updates (tau parameter)
#   6. SAC is more sample efficient for continuous actions
