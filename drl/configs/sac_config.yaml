# SAC Training Configuration for Traffic Intersection DRL - OPTIMIZED MODE

# Training Parameters - OPTIMIZED FOR CONTINUOUS ACTION SPACE
total_timesteps: 100000  # SAC is more sample efficient than PPO
eval_freq: 2000          # Evaluate every 2000 steps
checkpoint_freq: 5000    # Save checkpoints every 5000 steps

# SAC Algorithm Parameters - OPTIMIZED FOR TRAFFIC CONTROL
learning_rate: 3e-4      # Learning rate for both actor and critic
buffer_size: 1000000     # Experience replay buffer size (1M experiences)
batch_size: 256          # Batch size for training (larger than PPO for off-policy)
tau: 0.005               # Soft update coefficient for target networks
gamma: 0.99              # Discount factor
train_freq: 1            # Train every step (continuous learning)
gradient_steps: 1        # Number of gradient steps per training step
ent_coef: 'auto'         # Entropy coefficient (automatic tuning)
target_update_interval: 1 # Update target networks every step
learning_starts: 1000    # Start learning after 1000 random steps
use_sde: false           # Use State Dependent Exploration
use_action_noise: false  # Additional action noise (optional for SAC)

# Neural Network Architecture
policy_kwargs:
  log_std_init: -3       # Initial log std for policy
  net_arch: [256, 256]   # Hidden layers for actor and critic networks

# Simulation Configuration - FASTER EPISODES FOR SAC
sim_config:
  map: 'Town05'
  max_steps: 500         # Shorter episodes for SAC (off-policy can handle this better)
  spawn_rate: 0.8
  intersection_center: [-188.9, -89.7, 0.0]
  training_mode: true
  deadlock_reset_enabled: true
  deadlock_timeout_duration: 12.0  # Shorter timeout for SAC
  max_deadlock_resets: 3
  severe_deadlock_reset_enabled: true
  severe_deadlock_punishment: -300.0

# Reward Configuration - ADJUSTED FOR SAC
reward_config:
  vehicle_exit_reward: 10.0
  throughput_bonus: 0.01
  acceleration_penalty_threshold: 3.0
  acceleration_penalty_factor: 2.0
  efficiency_bonus: 5.0
  collision_penalty: 100.0
  deadlock_penalty: 400.0  # Lower than PPO since SAC learns from replay
  step_penalty: 0.05       # Lower step penalty for SAC

# Nash Conflict Parameters (trainable via DRL)
nash_hyperparams:
  path_intersection_threshold:
    range: [1.0, 5.0]  # Path intersection sensitivity (meters)
    default: 2.5
    description: "Distance threshold for detecting path intersections"
  platoon_conflict_distance:
    range: [5.0, 30.0]  # Platoon interaction distance (meters)  
    default: 15.0
    description: "Distance at which platoons are considered in conflict"

# SAC Specific Parameters
sac_specific:
  use_target_entropy: true    # Use target entropy for automatic entropy tuning
  target_entropy: 'auto'     # Target entropy (auto = -action_dim)
  ent_coef_lr: 3e-4          # Learning rate for entropy coefficient
  critic_lr: 3e-4            # Critic learning rate (can be different from actor)
  actor_lr: 3e-4             # Actor learning rate

# Training Schedule - DIFFERENT FROM PPO
schedule:
  warmup_steps: 1000         # Random exploration steps
  exploration_steps: 10000   # Steps for initial exploration
  min_learning_rate: 1e-5    # Minimum learning rate

# Evaluation
evaluation:
  n_eval_episodes: 5
  eval_deterministic: true
  eval_render: false

# Action Space Optimization for SAC
action_space:
  # SAC works better with normalized action spaces
  normalize_actions: true
  action_bounds_penalty: 0.1  # Penalty for actions near bounds

# Logging
logging:
  tensorboard: false
  csv: true
  metrics_freq: 100
  save_freq: 2000

# SAC vs PPO Differences:
# 1. SAC uses experience replay (buffer_size)
# 2. SAC is off-policy (can reuse old data)
# 3. SAC has automatic entropy tuning
# 4. SAC trains continuously (train_freq=1)
# 5. SAC uses soft updates (tau parameter)
# 6. SAC is more sample efficient for continuous actions
